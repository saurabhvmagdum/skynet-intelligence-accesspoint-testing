# GitHub Actions workflow for creating access_points table in Supabase

name: Sync Access Points 

on:
  # Trigger manually
  workflow_dispatch:
    inputs:
      operation:
        description: 'Operation to perform'
        required: true
        default: 'sync-access-points'
        type: choice
        options:
          - 'sync-access-points'
          - 'health-check'
  
  # Trigger on push to main when relevant files are modified
  push:
    branches: [ main ]
    paths:
      - 'src/accesspoint.ts'
      - 'src/dataSyncManager.ts'
      - 'src/pineconeOperations.ts'
      - 'src/supabaseOperations.ts'
      - 'supabase-schema.sql'

  pull_request:
    branches:
      - main

jobs:
  sync-access-points:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Setup Node.js
      uses: actions/setup-node@v4
      with:
        node-version: '18'
        cache: 'npm'
    
    - name: Install dependencies
      run: |
           npm install
           npm install -g ts-node typescript 
    
    - name: Build TypeScript
      run: npm run build
       
    - name: Install PostgreSQL Client
      run: |
        sudo apt-get update
        sudo apt-get install -y postgresql-client

    - name: Validate Environment Variables
      shell: bash
      run: |
        echo "Validating required environment variables..."
        ERROR_COUNT=0
        
        if [ -z "${{ secrets.DATABASE_URL }}" ]; then
          echo "❌ DATABASE_URL is not set"
          ERROR_COUNT=$((ERROR_COUNT + 1))
        else
          echo "✅ DATABASE_URL is set"
        fi
        
        if [ -z "${{ secrets.PINECONE_API_KEY }}" ]; then
          echo "❌ PINECONE_API_KEY is not set"
          ERROR_COUNT=$((ERROR_COUNT + 1))
        else
          echo "✅ PINECONE_API_KEY is set"
        fi
        
        if [ -z "${{ secrets.PINECONE_INDEX_NAME }}" ]; then
          echo "❌ PINECONE_INDEX_NAME is not set"
          ERROR_COUNT=$((ERROR_COUNT + 1))
        else
          echo "✅ PINECONE_INDEX_NAME is set"
        fi
        
        
        if [ $ERROR_COUNT -gt 0 ]; then
          echo "❌ $ERROR_COUNT environment variable(s) are missing"
          exit 1
        else 
          echo "✅ All required environment variables are set"
        fi
     
    - name: Test Database Connection
      env:
        DATABASE_URL: ${{ secrets.DATABASE_URL }}
      run: |
        echo "Testing database connection..."
        echo "Connection string format: postgresql://username:password@host:port/database"
        echo "Attempting to connect..."
        
        # Test connection with verbose output for debugging
        psql "$DATABASE_URL" -c "SELECT version();" -v ON_ERROR_STOP=1 || {
          echo "❌ Failed to connect to database"
          echo ""
          echo "🔍 Troubleshooting steps:"
          echo "1. Verify your DATABASE_URL format in GitHub secrets"
          echo "2. Check if the password contains special characters that need URL encoding"
          echo "3. Ensure the database user has the correct permissions"
          echo "4. Verify the connection string from your Supabase dashboard"
          echo ""
          echo "📋 Expected format:"
          echo "postgresql://postgres:[PASSWORD]@[HOST]:5432/postgres"
          echo ""
          echo "📋 For Supabase, get the connection string from:"
          echo "Project Settings > Database > Connection String > URI"
          echo ""
          echo "⚠️  Special characters in password may need URL encoding:"
          echo "@ → %40, # → %23, $ → %24, & → %26, + → %2B, etc."
          exit 1
        }
        echo "✅ Database connection successful"

    - name: Create Migration SQL File
      run: |
          echo "Creating migration SQL file..."
          TIMESTAMP=$(date +%Y%m%d%H%M%S)
          MIGRATION_FILE="create_access_points_${TIMESTAMP}.sql"
          echo "Creating migration file: $MIGRATION_FILE"
          cat > "$MIGRATION_FILE" << 'EOF'
          -- Migration: Create access_points table
          -- Generated by GitHub Actions
          
          -- Drop table if it exists (use with caution in production)
          -- DROP TABLE IF EXISTS access_points CASCADE;
          
          -- Create access_points table
          CREATE TABLE IF NOT EXISTS access_points (
              id VARCHAR(255) PRIMARY KEY,
              subnet_id VARCHAR(50) NOT NULL,
              subnet_name VARCHAR(100) NOT NULL,
              description TEXT NOT NULL,
              input VARCHAR(255) NOT NULL,
              output VARCHAR(255) NOT NULL,
              input_type VARCHAR(50) NOT NULL CHECK (input_type IN ('text', 'json', 'file', 'image')),
              output_type VARCHAR(50) NOT NULL CHECK (output_type IN ('text', 'json', 'file', 'image')),
              capabilities TEXT[] NOT NULL DEFAULT '{}',
              tags TEXT[] NOT NULL DEFAULT '{}',
              prompt_example TEXT,
              file_upload BOOLEAN NOT NULL DEFAULT FALSE,
              file_download BOOLEAN NOT NULL DEFAULT FALSE,
              subnet_url TEXT,
              created_at TIMESTAMPTZ DEFAULT NOW(),
              updated_at TIMESTAMPTZ DEFAULT NOW()
          );

          -- Create indexes for better query performance
          CREATE INDEX IF NOT EXISTS idx_access_points_subnet_id ON access_points(subnet_id);
          CREATE INDEX IF NOT EXISTS idx_access_points_subnet_name ON access_points(subnet_name);
          CREATE INDEX IF NOT EXISTS idx_access_points_tags ON access_points USING GIN(tags);
          CREATE INDEX IF NOT EXISTS idx_access_points_capabilities ON access_points USING GIN(capabilities);
          CREATE INDEX IF NOT EXISTS idx_access_points_input_type ON access_points(input_type);
          CREATE INDEX IF NOT EXISTS idx_access_points_output_type ON access_points(output_type);
          CREATE INDEX IF NOT EXISTS idx_access_points_created_at ON access_points(created_at);

          -- Create a function to automatically update the updated_at column
          CREATE OR REPLACE FUNCTION update_updated_at_column()
          RETURNS TRIGGER AS $$
          BEGIN
              NEW.updated_at = NOW();
              RETURN NEW;
          END;
          $$ language 'plpgsql';

          -- Create trigger to automatically update updated_at
          DROP TRIGGER IF EXISTS update_access_points_updated_at ON access_points;
          CREATE TRIGGER update_access_points_updated_at 
              BEFORE UPDATE ON access_points 
              FOR EACH ROW 
              EXECUTE FUNCTION update_updated_at_column();

          -- Optional: Create a function for full-text search
          CREATE OR REPLACE FUNCTION search_access_points(search_query TEXT)
          RETURNS TABLE (
              id VARCHAR(255),
              subnet_name VARCHAR(100),
              description TEXT,
              relevance_score REAL
          ) AS $$
          BEGIN
              RETURN QUERY
              SELECT 
                  ap.id,
                  ap.subnet_name,
                  ap.description,
                  ts_rank(
                      to_tsvector('english', ap.description || ' ' || array_to_string(ap.tags, ' ') || ' ' || array_to_string(ap.capabilities, ' ')),
                      plainto_tsquery('english', search_query)
                  ) AS relevance_score
              FROM access_points ap
              WHERE to_tsvector('english', ap.description || ' ' || array_to_string(ap.tags, ' ') || ' ' || array_to_string(ap.capabilities, ' '))
                    @@ plainto_tsquery('english', search_query)
              ORDER BY relevance_score DESC;
          END;
          $$ LANGUAGE plpgsql;

          -- Verify table creation
          SELECT 
              table_name,
              column_name,
              data_type,
              is_nullable,
              column_default
          FROM information_schema.columns 
          WHERE table_name = 'access_points' 
          ORDER BY ordinal_position;
          
          -- Show table info
          \d access_points;
          
          -- Success message
          SELECT 'access_points table created successfully!' as status;
          EOF
          echo "✅ Migration file created successfully: $MIGRATION_FILE"
      shell: bash

    - name: Apply Migration to Database
      env:
        DATABASE_URL: ${{ secrets.DATABASE_URL }}
      run: |
        echo "Applying migration to database..."
        MIGRATION_FILE="create_access_points_$(date +%Y%m%d%H%M%S).sql"
        
        # Execute the migration
        psql "$DATABASE_URL" -f "$MIGRATION_FILE" || {
          echo "❌ Migration failed"
          exit 1
        }
        
        echo "✅ Migration applied successfully"

    - name: Verify Table Creation
      env:
        DATABASE_URL: ${{ secrets.DATABASE_URL }}
      run: |
        echo "Verifying table creation..."
        
        # Check if table exists
        TABLE_EXISTS=$(psql "$DATABASE_URL" -t -c "SELECT EXISTS (SELECT FROM information_schema.tables WHERE table_name = 'access_points');")
        
        if [[ "$TABLE_EXISTS" == *"t"* ]]; then
          echo "✅ Table 'access_points' exists"
          
          # Get table structure
          echo "📋 Table structure:"
          psql "$DATABASE_URL" -c "\d access_points"
          
          # Get indexes
          echo "📋 Table indexes:"
          psql "$DATABASE_URL" -c "SELECT indexname, indexdef FROM pg_indexes WHERE tablename = 'access_points';"
          
          # Get triggers
          echo "📋 Table triggers:"
          psql "$DATABASE_URL" -c "SELECT trigger_name, event_manipulation, action_statement FROM information_schema.triggers WHERE event_object_table = 'access_points';"
          
        else
          echo "❌ Table 'access_points' does not exist"
          exit 1
        fi

    - name: Run Health Check
      env:
        DATABASE_URL: ${{ secrets.DATABASE_URL }}
        PINECONE_API_KEY: ${{ secrets.PINECONE_API_KEY }}
        PINECONE_INDEX_NAME: ${{ secrets.PINECONE_INDEX_NAME }}
        SUPABASE_KEY: ${{ secrets.SUPABASE_KEY }}
        SUPABASE_URL: ${{ secrets.SUPABASE_URL}}
        OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
      run: |
        echo "Running health check..."
        # Test database connection
        psql "$DATABASE_URL" -c "SELECT COUNT(*) FROM access_points;" || {
          echo "❌ Health check failed - cannot query access_points table"
          exit 1
        }
        
        # Run TypeScript health check if available
        if [ -f "src/dataSyncManager.ts" ]; then
          ts-node src/dataSyncManager.ts
        fi
        
        echo "✅ Health check completed"
 
    - name: Run Data Sync
      env:
        DATABASE_URL: ${{ secrets.DATABASE_URL }}
        PINECONE_API_KEY: ${{ secrets.PINECONE_API_KEY }}
        PINECONE_INDEX_NAME: ${{ secrets.PINECONE_INDEX_NAME }}
        SUPABASE_KEY: ${{ secrets.SUPABASE_KEY }}
        SUPABASE_URL: ${{ secrets.SUPABASE_URL}}
        OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
      run: |
        echo "Running data sync..."
        if [ -f "src/dataSyncManager.ts" ]; then
          ts-node src/dataSyncManager.ts
        else
          echo "⚠️  dataSyncManager.ts not found, skipping sync"
        fi

    - name: Get current date
      id: date
      run: echo "date=$(date '+%Y-%m-%d %H:%M:%S')" >> $GITHUB_OUTPUT

    - name: Notify on Failure
      if: failure()
      uses: actions/github-script@v7
      with:
        script: |
          const operation = context.payload.inputs?.operation || 'triggered by push';
          const runUrl = `https://github.com/${context.repo.owner}/${context.repo.repo}/actions/runs/${context.runId}`;
          
          const issueBody = `## Database Table Creation Failure
          
          **Operation**: "${operation}"  
          **Triggered by**: ${context.actor}  
          **Workflow Run**: [View Logs](${runUrl})  
          **Time**: ${{ steps.date.outputs.date }}
          
          ## Please check the logs for more details.
            
          ### Possible causes:
          - Invalid DATABASE_URL format
          - Database connection issues
          - Insufficient database permissions
          - Invalid SQL in the migration file
          - Missing Pinecone or OpenAI API keys (for sync operations)
          
          ### DATABASE_URL Format Should Be:
          \`postgresql://username:password@host:port/database\`
           
          ### Required Permissions:
          - CREATE TABLE
          - CREATE INDEX
          - CREATE FUNCTION
          - CREATE TRIGGER`;
          
          await github.rest.issues.create({
            owner: context.repo.owner,
            repo: context.repo.repo,
            title: `Database Table Creation Failed: ${operation}`,
            body: issueBody
          });
      
permissions:
  issues: write
  contents: read