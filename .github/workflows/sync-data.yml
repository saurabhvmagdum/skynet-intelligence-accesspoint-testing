# GitHub Actions workflow for syncing data between Supabase and Pinecone

name: Sync Access Points Data

on:
  # Trigger manually
  workflow_dispatch:
    inputs:
      operation:
        description: 'Operation to perform'
        required: true
        default: 'health-check'
        type: choice
        options:
          - 'health-check'
          - 'sync-only'
  
  # Trigger on push to main when relevant files are modified
  push:
    branches: [ main ]
    paths:
      - 'src/accesspoint.ts'
      - 'src/dataSyncManager.ts'
      - 'src/pineconeOperations.ts'
      - 'src/supabaseOperations.ts'
      - 'supabase-schema.sql' # Include migration file path

  pull_request:
    branches:
      - main

jobs:
  sync-and-create:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Setup Node.js
      uses: actions/setup-node@v4
      with:
        node-version: '18'
        cache: 'npm'
    
    - name: Install dependencies
      run: |
           npm install
           npm install -g ts-node typescript 

    - name: Build TypeScript
      run: npm run build
      
    - name: Install Supabase CLI
      run: |
        curl -sSfL https://github.com/supabase/cli/releases/latest/download/supabase_linux_amd64.tar.gz | tar -xz
        sudo mv supabase /usr/local/bin/

    - name: Authenticate with Supabase
      env:
        SUPABASE_KEY: ${{ secrets.SUPABASE_KEY }}
      run: supabase login --service-key $SUPABASE_KEY

    - name: Create migration file
      run: |
          mkdir -p supabase/migrations # Ensure migrations directory exists
          TIMESTAMP=$(date +%Y%m%d%H%M%S)
          MIGRATION_FILE="supabase/migrations/${TIMESTAMP}_init.sql"
          echo "Creating migration file: $MIGRATION_FILE"

          echo "
          -- This migration was auto-generated by a GitHub Action
          -- Creating access_points table
          CREATE TABLE IF NOT EXISTS access_points (
              id VARCHAR(255) PRIMARY KEY,
              subnet_id VARCHAR(50) NOT NULL,
              subnet_name VARCHAR(100) NOT NULL,
              description TEXT NOT NULL,
              input VARCHAR(255) NOT NULL,
              output VARCHAR(255) NOT NULL,
              input_type VARCHAR(50) NOT NULL CHECK (input_type IN ('text', 'json', 'file', 'image')),
              output_type VARCHAR(50) NOT NULL CHECK (output_type IN ('text', 'json', 'file', 'image')),
              capabilities TEXT[] NOT NULL DEFAULT '{}',
              tags TEXT[] NOT NULL DEFAULT '{}',
              prompt_example TEXT,
              file_upload BOOLEAN NOT NULL DEFAULT FALSE,
              file_download BOOLEAN NOT NULL DEFAULT FALSE,
              subnet_url TEXT,
              created_at TIMESTAMPTZ DEFAULT NOW(),
              updated_at TIMESTAMPTZ DEFAULT NOW()
          );

          -- Create indexes for better query performance
          CREATE INDEX IF NOT EXISTS idx_access_points_subnet_id ON access_points(subnet_id);
          CREATE INDEX IF NOT EXISTS idx_access_points_subnet_name ON access_points(subnet_name);
          CREATE INDEX IF NOT EXISTS idx_access_points_tags ON access_points USING GIN(tags);
          CREATE INDEX IF NOT EXISTS idx_access_points_capabilities ON access_points USING GIN(capabilities);
          CREATE INDEX IF NOT EXISTS idx_access_points_input_type ON access_points(input_type);
          CREATE INDEX IF NOT EXISTS idx_access_points_output_type ON access_points(output_type);
          CREATE INDEX IF NOT EXISTS idx_access_points_created_at ON access_points(created_at);

          -- Create a function to automatically update the updated_at column
          CREATE OR REPLACE FUNCTION update_updated_at_column()
          RETURNS TRIGGER AS $$
          BEGIN
              NEW.updated_at = NOW();
              RETURN NEW;
          END;
          $$ language 'plpgsql';

          -- Create trigger to automatically update updated_at
          CREATE TRIGGER update_access_points_updated_at 
              BEFORE UPDATE ON access_points 
              FOR EACH ROW 
              EXECUTE FUNCTION update_updated_at_column();

          -- Enable Row Level Security (RLS) - Optional but recommended
          ALTER TABLE access_points ENABLE ROW LEVEL SECURITY;

          -- Create policies (adjust based on your authentication needs)
          -- Allow all operations for authenticated users
          CREATE POLICY "Allow all operations for authenticated users" ON access_points
              FOR ALL USING (auth.role() = 'authenticated');

          -- Allow read access for anonymous users (optional)
          CREATE POLICY "Allow read access for anonymous users" ON access_points
              FOR SELECT USING (true);

          -- Grant necessary permissions (adjust based on your needs)
          GRANT SELECT, INSERT, UPDATE, DELETE ON access_points TO authenticated;

          -- Optional: Create a function for full-text search
          CREATE OR REPLACE FUNCTION search_access_points(search_query TEXT)
          RETURNS TABLE (
              id VARCHAR(255),
              subnet_name VARCHAR(100),
              description TEXT,
              relevance_score REAL
          ) AS $$
          BEGIN
              RETURN QUERY
              SELECT 
                  ap.id,
                  ap.subnet_name,
                  ap.description,
                  ts_rank(
                      to_tsvector('english', ap.description || ' ' || array_to_string(ap.tags, ' ') || ' ' || array_to_string(ap.capabilities, ' ')),
                      plainto_tsquery('english', search_query)
                  ) AS relevance_score
              FROM access_points ap
              WHERE to_tsvector('english', ap.description || ' ' || array_to_string(ap.tags, ' ') || ' ' || array_to_string(ap.capabilities, ' '))
                    @@ plainto_tsquery('english', search_query)
              ORDER BY relevance_score DESC;
          END;
          $$ LANGUAGE plpgsql;
          " > "$MIGRATION_FILE"
          echo "Migration file created successfully."
      shell: bash

    - name: Apply migration
      run: supabase db push

    - name: Validate Environment Variables
      if: always() # Run even if previous steps fail
      shell: bash
      run: |
        echo "Validating required environment variables..."
        if [ -z "${{ secrets.DATABASE_URL }}" ]; then
          echo "DATABASE_URL is not set"
          exit 1
        fi
        if [ -z "${{ secrets.SUPABASE_KEY }}" ]; then
          echo "SUPABASE_KEY is not set"
          exit 1
        fi
        if [ -z "${{ secrets.PINECONE_API_KEY }}" ]; then
          echo "PINECONE_API_KEY is not set"
          exit 1
        fi
        if [ -z "${{ secrets.PINECONE_INDEX_NAME }}" ]; then
          echo "PINECONE_INDEX_NAME is not set"
          exit 1
        fi
        echo "All required environment variables are set"
    
    - name: Run Health Check
      if: github.event.inputs.operation == 'health-check' || github.event_name == 'schedule'
      env:
        DATABASE_URL: ${{ secrets.DATABASE_URL }}
        SUPABASE_KEY: ${{ secrets.SUPABASE_KEY }}
        PINECONE_API_KEY: ${{ secrets.PINECONE_API_KEY }}
        PINECONE_INDEX_NAME: ${{ secrets.PINECONE_INDEX_NAME }}
        OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
      run: ts-node src/dataSyncManager.ts
    
    - name: Sync Data
      if: github.event.inputs.operation == 'sync-only' || github.event_name == 'push'
      env:
        DATABASE_URL: ${{ secrets.DATABASE_URL }}
        SUPABASE_KEY: ${{ secrets.SUPABASE_KEY }}
        PINECONE_API_KEY: ${{ secrets.PINECONE_API_KEY }}
        PINECONE_INDEX_NAME: ${{ secrets.PINECONE_INDEX_NAME }}
        OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
      run: ts-node src/dataSyncManager.ts

    - name: Get current date
      id: date
      run: echo "::set-output name=date::$(date '+%Y-%m-%d %H:%M:%S')"

    - name: Notify on Failure
      if: failure()
      uses: actions/github-script@v7
      with:
        script: |
          const operation = context.payload.inputs?.operation || 'triggered by push';
          const runUrl = `https://github.com/${context.repo.owner}/${context.repo.repo}/actions/runs/${context.runId}`;
          
          const issueBody = `## Data Sync or Table Creation Failure

          Operation: "${operation}"
          Triggered by: ${context.actor}
          Workflow Run: [View Logs](${runUrl})
          Time: ${{ steps.date.outputs.date }}

          ## Please check the logs for more details and ensure all environment variables are properly configured.  Possible causes:

          *   Incorrect Supabase service account key.
          *   Database connection issues.
          *   Invalid SQL in the migration file.
          *   Missing Pinecone or OpenAI API keys.`;

          await github.rest.issues.create({
            owner: context.repo.owner,
            repo: context.repo.repo,
            title: `Data Sync/Table Creation Failed: ${operation}`,
            body: issueBody
          });
      
permissions:
    issues: write
    contents: read